{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Started.\n",
      "Spark Version: 3.5.7\n",
      "------------------------------------------------------------\n",
      "Schema:\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Training samples: 126, Test samples: 24\n",
      "=== Predictions Sample ===\n",
      "+-------+-----+----------+-----------------------------------------------------------------+\n",
      "|species|label|prediction|probability                                                      |\n",
      "+-------+-----+----------+-----------------------------------------------------------------+\n",
      "|setosa |2.0  |2.0       |[2.924871346249377E-7,2.1723569697143757E-30,0.9999997075128654] |\n",
      "|setosa |2.0  |2.0       |[3.5370707821178566E-7,1.0398673899064905E-30,0.9999996462929218]|\n",
      "|setosa |2.0  |2.0       |[2.3454927323176524E-9,1.3038338868246855E-35,0.9999999976545073]|\n",
      "|setosa |2.0  |2.0       |[1.1010969837461104E-5,2.4201918582320394E-28,0.9999889890301625]|\n",
      "|setosa |2.0  |2.0       |[1.1752031049167168E-5,1.6325308368040156E-29,0.9999882479689509]|\n",
      "+-------+-----+----------+-----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Model Accuracy = 1.0000\n",
      "------------------------------------------------------------\n",
      "Cross-Validated Accuracy = 1.0000\n",
      "âœ… Model saved to output/iris_logreg_pipeline\n",
      "âœ… Model pipeline reloaded successfully.\n",
      "ðŸ§  Spark session stopped. End of Lesson 5.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lesson 5: Machine Learning Pipelines in PySpark\n",
    "===============================================\n",
    "\n",
    "Author: Deb\n",
    "Date:   2024-06-10\n",
    "\n",
    "Description:\n",
    "------------\n",
    "End-to-end supervised classification example using PySpark ML pipelines on the Iris dataset.\n",
    "\n",
    "Key Concepts:\n",
    "-------------\n",
    "1. Feature transformation with VectorAssembler & StandardScaler\n",
    "2. Training a Logistic Regression classifier\n",
    "3. Building a Pipeline with multiple stages\n",
    "4. Model evaluation using accuracy metrics\n",
    "5. Hyperparameter tuning via CrossValidator\n",
    "6. Saving and loading trained models\n",
    "\n",
    "Dataset:\n",
    "--------\n",
    "Iris dataset (UCI Machine Learning Repository)\n",
    "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "\n",
    "Run:\n",
    "----\n",
    "$ source venv/bin/activate\n",
    "$ python src/lesson5_ml_pipeline.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. Initialize Spark Session\n",
    "# =========================================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lesson5-ml-pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Started.\")\n",
    "print(\"Spark Version:\", spark.version)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Download and Load Dataset\n",
    "# =========================================================\n",
    "iris_url = \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\"\n",
    "local_path = Path(\"/tmp/iris.csv\")\n",
    "urllib.request.urlretrieve(iris_url, local_path)\n",
    "\n",
    "iris_df = spark.read.csv(str(local_path), header=True, inferSchema=True)\n",
    "print(\"Schema:\")\n",
    "iris_df.printSchema()\n",
    "iris_df.show(5)\n",
    "\n",
    "# =========================================================\n",
    "# 3. Data Preparation\n",
    "# =========================================================\n",
    "# Convert categorical 'species' into numeric label (index)\n",
    "label_indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "\n",
    "# Combine numerical features into a single vector column\n",
    "feature_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "\n",
    "# Optionally scale the features\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# =========================================================\n",
    "# 4. Define Model (Estimator)\n",
    "# =========================================================\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
    "\n",
    "# =========================================================\n",
    "# 5. Build ML Pipeline\n",
    "# =========================================================\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler, scaler, lr])\n",
    "\n",
    "# =========================================================\n",
    "# 6. Split Data into Train/Test\n",
    "# =========================================================\n",
    "train_df, test_df = iris_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training samples: {train_df.count()}, Test samples: {test_df.count()}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7. Fit Model\n",
    "# =========================================================\n",
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "print(\"=== Predictions Sample ===\")\n",
    "predictions.select(\"species\", \"label\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "# =========================================================\n",
    "# 8. Evaluate Model\n",
    "# =========================================================\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy = {accuracy:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# =========================================================\n",
    "# 9. Hyperparameter Tuning (Cross Validation)\n",
    "# =========================================================\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_model = crossval.fit(train_df)\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "print(f\"Cross-Validated Accuracy = {cv_accuracy:.4f}\")\n",
    "\n",
    "# =========================================================\n",
    "# 10. Save & Load Model\n",
    "# =========================================================\n",
    "model_path = \"output/iris_logreg_pipeline\"\n",
    "cv_model.bestModel.write().overwrite().save(model_path)\n",
    "print(f\"âœ… Model saved to {model_path}\")\n",
    "\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "print(\"âœ… Model pipeline reloaded successfully.\")\n",
    "\n",
    "# =========================================================\n",
    "# 11. Stop Spark\n",
    "# =========================================================\n",
    "spark.stop()\n",
    "print(\"ðŸ§  Spark session stopped. End of Lesson 5.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
