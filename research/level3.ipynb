{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631abe13",
   "metadata": {},
   "source": [
    "DataFrame Operations & Transformations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7ca46",
   "metadata": {},
   "source": [
    "By the end of this step, youâ€™ll learn:\n",
    "\n",
    "How to read and write data in CSV, JSON, and Parquet formats.\n",
    "\n",
    "How to perform select, filter, groupBy, and joins using the DataFrame API.\n",
    "\n",
    "How Spark SQL fits in â€” you can mix SQL and DataFrame APIs.\n",
    "\n",
    "Best practices: schema inference, repartitioning, and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69419c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session initialized successfully.\n",
      "Spark Version: 3.5.7\n",
      "------------------------------------------------------------\n",
      "â¬‡ï¸  Downloading Iris dataset...\n",
      "âœ… Downloaded dataset to: /tmp/iris.csv\n",
      "------------------------------------------------------------\n",
      "=== Schema of Iris Data ===\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "=== First 5 Rows ===\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Selecting 'sepal_length' and 'species' columns ===\n",
      "+------------+-------+\n",
      "|sepal_length|species|\n",
      "+------------+-------+\n",
      "|         5.1| setosa|\n",
      "|         4.9| setosa|\n",
      "|         4.7| setosa|\n",
      "|         4.6| setosa|\n",
      "|         5.0| setosa|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Filtering rows where species == 'setosa' ===\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Filtering rows where sepal_length > 5.0 and species == 'versicolor' ===\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "|         7.0|        3.2|         4.7|        1.4|versicolor|\n",
      "|         6.4|        3.2|         4.5|        1.5|versicolor|\n",
      "|         6.9|        3.1|         4.9|        1.5|versicolor|\n",
      "|         5.5|        2.3|         4.0|        1.3|versicolor|\n",
      "|         6.5|        2.8|         4.6|        1.5|versicolor|\n",
      "+------------+-----------+------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Added Derived Column: 'petal_area' ===\n",
      "+-------+------------+-----------+-------------------+\n",
      "|species|petal_length|petal_width|         petal_area|\n",
      "+-------+------------+-----------+-------------------+\n",
      "| setosa|         1.4|        0.2|0.27999999999999997|\n",
      "| setosa|         1.4|        0.2|0.27999999999999997|\n",
      "| setosa|         1.3|        0.2|               0.26|\n",
      "| setosa|         1.5|        0.2|0.30000000000000004|\n",
      "| setosa|         1.4|        0.2|0.27999999999999997|\n",
      "+-------+------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Aggregated Averages by Species ===\n",
      "+----------+-----------------+-------------+\n",
      "|   species|    avg_sepal_len|avg_petal_len|\n",
      "+----------+-----------------+-------------+\n",
      "| virginica|6.587999999999998|        5.552|\n",
      "|versicolor|            5.936|         4.26|\n",
      "|    setosa|5.005999999999999|        1.464|\n",
      "+----------+-----------------+-------------+\n",
      "\n",
      "=== Sorting by sepal_length (descending) ===\n",
      "+------------+-----------+------------+-----------+---------+------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  species|        petal_area|\n",
      "+------------+-----------+------------+-----------+---------+------------------+\n",
      "|         7.9|        3.8|         6.4|        2.0|virginica|              12.8|\n",
      "|         7.7|        3.8|         6.7|        2.2|virginica|14.740000000000002|\n",
      "|         7.7|        2.8|         6.7|        2.0|virginica|              13.4|\n",
      "|         7.7|        2.6|         6.9|        2.3|virginica|             15.87|\n",
      "|         7.7|        3.0|         6.1|        2.3|virginica|14.029999999999998|\n",
      "+------------+-----------+------------+-----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Joined DataFrame ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+------------+\n",
      "|species| description|sepal_length|petal_length|\n",
      "+-------+------------+------------+------------+\n",
      "| setosa|short petals|         5.0|         1.4|\n",
      "| setosa|short petals|         5.3|         1.5|\n",
      "| setosa|short petals|         4.6|         1.4|\n",
      "| setosa|short petals|         5.1|         1.6|\n",
      "| setosa|short petals|         4.8|         1.4|\n",
      "+-------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== Writing Data to Disk (CSV, Parquet, JSON) ===\n",
      "âœ… Data successfully written to output/ folder.\n",
      "------------------------------------------------------------\n",
      "=== SQL Query Results ===\n",
      "+----------+-----------------+----------------+\n",
      "|   species| avg_sepal_length|avg_petal_length|\n",
      "+----------+-----------------+----------------+\n",
      "| virginica|6.587999999999998|           5.552|\n",
      "|versicolor|            5.936|            4.26|\n",
      "|    setosa|5.005999999999999|           1.464|\n",
      "+----------+-----------------+----------------+\n",
      "\n",
      "ðŸ§  Spark session stopped. End of Lesson 3.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lesson 3: DataFrame Operations and Transformations in PySpark\n",
    "=============================================================\n",
    "\n",
    "Author: Deb\n",
    "Date: 2024-06-15\n",
    "\n",
    "Description:\n",
    "------------\n",
    "This script demonstrates essential PySpark DataFrame operations using the Iris dataset.\n",
    "It includes downloading the dataset locally, reading it into a Spark DataFrame,\n",
    "and performing transformations, aggregations, joins, and SQL queries.\n",
    "\n",
    "Covered Topics:\n",
    "---------------\n",
    "1. Creating a SparkSession\n",
    "2. Downloading data programmatically using urllib\n",
    "3. Reading CSV data into a DataFrame\n",
    "4. Performing DataFrame operations:\n",
    "   - Selecting and filtering columns\n",
    "   - Adding derived columns\n",
    "   - Grouping and aggregations\n",
    "   - Sorting and ordering\n",
    "   - Joining DataFrames\n",
    "5. Writing data to Parquet, CSV, and JSON\n",
    "6. Executing SQL queries on Spark DataFrames\n",
    "\n",
    "Dataset Used:\n",
    "-------------\n",
    "Iris dataset (UCI Machine Learning Repository)\n",
    "Source: https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "\n",
    "Usage:\n",
    "------\n",
    "1. Activate your virtual environment:\n",
    "   $ source venv/bin/activate\n",
    "\n",
    "2. Run the script:\n",
    "   $ python src/lesson3_dataframe_ops.py\n",
    "\n",
    "Output:\n",
    "-------\n",
    "- Downloads the Iris dataset to /tmp/iris.csv\n",
    "- Displays sample transformations in console\n",
    "- Writes transformed DataFrames to output directories (Parquet, CSV, JSON)\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# 1. Import required libraries\n",
    "# ===============================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Create SparkSession\n",
    "# ===============================\n",
    "# SparkSession is the unified entry point for Spark operations such as\n",
    "# reading data, performing transformations, and running SQL queries.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"lesson3-dataframe-ops\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session initialized successfully.\")\n",
    "print(\"Spark Version:\", spark.version)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Download Iris CSV dataset locally\n",
    "# ===============================\n",
    "# This ensures reproducibility and allows the script to run even offline.\n",
    "# The dataset will be saved to /tmp/iris.csv (works on macOS/Linux).\n",
    "# On Windows, adjust to Path.home() / \"iris.csv\" if needed.\n",
    "\n",
    "iris_url = \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\"\n",
    "local_path = Path(\"/tmp/iris.csv\")\n",
    "\n",
    "print(\"â¬‡ï¸  Downloading Iris dataset...\")\n",
    "urllib.request.urlretrieve(iris_url, local_path)\n",
    "print(f\"âœ… Downloaded dataset to: {local_path}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Read CSV file into DataFrame\n",
    "# ===============================\n",
    "# inferSchema=True allows Spark to automatically infer column data types.\n",
    "\n",
    "iris_df = spark.read.csv(str(local_path), header=True, inferSchema=True)\n",
    "\n",
    "print(\"=== Schema of Iris Data ===\")\n",
    "iris_df.printSchema()\n",
    "\n",
    "print(\"=== First 5 Rows ===\")\n",
    "iris_df.show(5)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. Select & Filter Columns\n",
    "# ===============================\n",
    "# Example of basic transformations and filters in PySpark DataFrames.\n",
    "\n",
    "print(\"=== Selecting 'sepal_length' and 'species' columns ===\")\n",
    "iris_df.select(\"sepal_length\", \"species\").show(5)\n",
    "\n",
    "print(\"=== Filtering rows where species == 'setosa' ===\")\n",
    "iris_df.filter(iris_df[\"species\"] == \"setosa\").show(5)\n",
    "\n",
    "print(\"=== Filtering rows where sepal_length > 5.0 and species == 'versicolor' ===\")\n",
    "iris_df.filter((iris_df[\"sepal_length\"] > 5.0) & (iris_df[\"species\"] == \"versicolor\")).show(5)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Add Derived Column (Feature Engineering)\n",
    "# ===============================\n",
    "# Create a new column 'petal_area' = petal_length * petal_width.\n",
    "\n",
    "iris_df = iris_df.withColumn(\"petal_area\", col(\"petal_length\") * col(\"petal_width\"))\n",
    "\n",
    "print(\"=== Added Derived Column: 'petal_area' ===\")\n",
    "iris_df.select(\"species\", \"petal_length\", \"petal_width\", \"petal_area\").show(5)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. GroupBy and Aggregations\n",
    "# ===============================\n",
    "# Compute average sepal and petal lengths per species.\n",
    "\n",
    "agg_df = iris_df.groupBy(\"species\").agg(\n",
    "    F.avg(\"sepal_length\").alias(\"avg_sepal_len\"),\n",
    "    F.avg(\"petal_length\").alias(\"avg_petal_len\")\n",
    ")\n",
    "\n",
    "print(\"=== Aggregated Averages by Species ===\")\n",
    "agg_df.show()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 8. Sorting and Ordering\n",
    "# ===============================\n",
    "# Sort by sepal_length in descending order.\n",
    "\n",
    "print(\"=== Sorting by sepal_length (descending) ===\")\n",
    "iris_df.orderBy(\"sepal_length\", ascending=False).show(5)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 9. Joining DataFrames\n",
    "# ===============================\n",
    "# Demonstrate joining two DataFrames on a common key (species).\n",
    "\n",
    "species_data = [\n",
    "    (\"setosa\", \"short petals\"),\n",
    "    (\"versicolor\", \"medium petals\"),\n",
    "    (\"virginica\", \"long petals\"),\n",
    "]\n",
    "\n",
    "species_df = spark.createDataFrame(species_data, [\"species\", \"description\"])\n",
    "\n",
    "# Inner join between iris_df and species_df.\n",
    "joined_df = iris_df.join(species_df, on=\"species\", how=\"inner\")\n",
    "\n",
    "print(\"=== Joined DataFrame ===\")\n",
    "joined_df.select(\"species\", \"description\", \"sepal_length\", \"petal_length\").show(5)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 10. Write Output Data\n",
    "# ===============================\n",
    "# Spark writes distributed files, so each output is a directory with multiple part files.\n",
    "\n",
    "print(\"=== Writing Data to Disk (CSV, Parquet, JSON) ===\")\n",
    "\n",
    "joined_df.write.mode(\"overwrite\").parquet(\"output/iris_parquet\")\n",
    "joined_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"output/iris_csv\")\n",
    "joined_df.write.mode(\"overwrite\").json(\"output/iris_json\")\n",
    "\n",
    "print(\"âœ… Data successfully written to output/ folder.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 11. SQL Interface Example\n",
    "# ===============================\n",
    "# Register the DataFrame as a temporary SQL table and run queries directly.\n",
    "\n",
    "iris_df.createOrReplaceTempView(\"iris\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    species,\n",
    "    AVG(sepal_length) AS avg_sepal_length,\n",
    "    AVG(petal_length) AS avg_petal_length\n",
    "FROM iris\n",
    "GROUP BY species\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL Query Results ===\")\n",
    "sql_result.show()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 12. Stop the Spark Session\n",
    "# ===============================\n",
    "# Always stop Spark when done to free resources.\n",
    "\n",
    "spark.stop()\n",
    "print(\"ðŸ§  Spark session stopped. End of Lesson 3.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
