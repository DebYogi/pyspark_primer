{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08374a25",
   "metadata": {},
   "source": [
    " Referring to Columns in PySpark\n",
    "In PySpark, referencing columns is essential for filtering, selecting, transforming, and performing other DataFrame operations. Unlike SQL, PySpark provides several options for referring to columns, each suited to different tasks. Letâ€™s explore these approaches with examples across common operations, such as filtering, selecting, and applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a8e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/07 18:30:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/07 18:30:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/07 18:30:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/07 18:30:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark's in the house! ðŸ”¥\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initiate the SparkSession - you're basically summoning Spark's power!\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark 101\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark's in the house! ðŸ”¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ca845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14| 21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03| 34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94| 33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33| 36.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read employee data\n",
    "df = spark.read.csv(\"../data/boston.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d6841dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   CRIM|   RM|\n",
      "+-------+-----+\n",
      "|0.00632|6.575|\n",
      "|0.02731|6.421|\n",
      "+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns by name\n",
    "df.select(\"CRIM\", \"RM\").show(2)\n",
    "\n",
    "# Filtering based on a column condition\n",
    "df.filter(\"AGE > 30\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9caf089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   CRIM|   RM|\n",
      "+-------+-----+\n",
      "|0.00632|6.575|\n",
      "|0.02731|6.421|\n",
      "+-------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns using dot notation\n",
    "df.select(df.CRIM, df.RM).show(2)\n",
    "\n",
    "# Filtering rows based on column conditions\n",
    "df.filter(df.AGE > 30).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245bba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   CRIM| AGE|\n",
      "+-------+----+\n",
      "|0.00632|65.2|\n",
      "|0.02731|78.9|\n",
      "+-------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Selecting columns using col()\n",
    "df.select(col(\"CRIM\"), col(\"AGE\")).show(2)\n",
    "\n",
    "# Filtering rows using col() for flexibility\n",
    "age_column = \"AGE\"\n",
    "df.filter(col(age_column) > 30).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b07c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   CRIM| AGE|\n",
      "+-------+----+\n",
      "|0.00632|65.2|\n",
      "|0.02731|78.9|\n",
      "+-------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------+\n",
      "|age_plus_10|\n",
      "+-----------+\n",
      "|       75.2|\n",
      "|       88.9|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns\n",
    "df.select(df[\"CRIM\"], df[\"AGE\"]).show(2)\n",
    "\n",
    "# Filtering with expressions\n",
    "df.filter(df[\"AGE\"] > 30).show(2)\n",
    "\n",
    "# Applying transformations\n",
    "df.select((df[\"age\"] + 10).alias(\"age_plus_10\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ab703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   CRIM|constant_age|\n",
      "+-------+------------+\n",
      "|0.00632|          25|\n",
      "|0.02731|          25|\n",
      "+-------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Adding a constant column\n",
    "df.select(df.CRIM, lit(25).alias(\"constant_age\")).show(2)\n",
    "\n",
    "# Filtering based on a constant\n",
    "df.filter(df.AGE > lit(30)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a225ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|   CRIM| AGE|  NOX|\n",
      "+-------+----+-----+\n",
      "|0.00632|65.2|0.538|\n",
      "|0.02731|78.9|0.469|\n",
      "+-------+----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting a single column\n",
    "selected_df = df.select(\"CRIM\")\n",
    "\n",
    "# Selecting multiple columns\n",
    "selected_df = df.select(\"CRIM\", \"AGE\", \"NOX\")\n",
    "selected_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8162ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|   CRIM| AGE|  NOX|\n",
      "+-------+----+-----+\n",
      "|0.00632|65.2|0.538|\n",
      "|0.02731|78.9|0.469|\n",
      "+-------+----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Selecting columns using col()\n",
    "selected_df = df.select(col(\"CRIM\"), col(\"AGE\"), col(\"NOX\"))\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e675e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|  CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|active|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|active| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+----------+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|age_plus_5|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+----------+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|      70.2|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|      83.9|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adding a new column with a static value\n",
    "df_with_constant = df.withColumn(\"CRIM\", lit(\"active\"))\n",
    "\n",
    "# Adding a new column based on an existing one\n",
    "df_with_computed = df.withColumn(\"age_plus_5\", col(\"AGE\") + 5)\n",
    "df_with_constant.show(2)\n",
    "df_with_computed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a325c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------------------+\n",
      "|location_crim|age_of_property|nitric_oxide_level|\n",
      "+-------------+---------------+------------------+\n",
      "|      0.00632|           65.2|             0.538|\n",
      "|      0.02731|           78.9|             0.469|\n",
      "+-------------+---------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns\n",
    "selected_df = df.select(col(\"CRIM\").alias(\"location_crim\"), col(\"AGE\").alias(\"age_of_property\"), col(\"NOX\").alias(\"nitric_oxide_level\"))\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5749460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|next_year_age|Price|\n",
      "+-------------+-----+\n",
      "|         66.2| 24.0|\n",
      "|         79.9| 21.6|\n",
      "+-------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Selecting with expressions\n",
    "selected_df = df.select(expr(\"AGE + 1 AS next_year_age\"), \"Price\")\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf4ec5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting all columns except 'salary'\n",
    "selected_df = df.select(\"*\").drop(\"CRIM\")\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b1c0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   CRIM| AGE|\n",
      "+-------+----+\n",
      "|0.00632|65.2|\n",
      "|0.02731|78.9|\n",
      "+-------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"Boston\")\n",
    "\n",
    "# Using SQL to select columns\n",
    "selected_df = spark.sql(\"SELECT CRIM, AGE FROM Boston\")\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4744e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   CRIM| AGE|\n",
      "+-------+----+\n",
      "|0.00632|65.2|\n",
      "|0.02731|78.9|\n",
      "+-------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to select\n",
    "columns_to_select = [\"CRIM\", \"AGE\"]\n",
    "\n",
    "# Dynamically selecting columns\n",
    "selected_df = df.select(*columns_to_select)\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db932ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+-----+------+---+---+-------+-----+-----+-----+\n",
      "|  ZN|INDUS|CHAS|  NOX|   RM|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+----+-----+----+-----+-----+------+---+---+-------+-----+-----+-----+\n",
      "|18.0| 2.31|   0|0.538|6.575|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "| 0.0| 7.07|   0|0.469|6.421|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+----+-----+----+-----+-----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keeping all columns except 'salary' and 'address'\n",
    "selected_df = df.drop(\"AGE\", \"CRIM\")\n",
    "selected_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9852e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of basic filtering\n",
    "filtered_df = df.filter(df[\"AGE\"] > 30)\n",
    "# or\n",
    "filtered_df = df.where(\"AGE > 30\")\n",
    "\n",
    "filtered_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5882cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND Condition (&):\n",
    "\n",
    "filtered_df = df.filter((df[\"AGE\"] > 30) & (df[\"CRIM\"] > 50000))\n",
    "\n",
    "# OR Condition (|):\n",
    "\n",
    "filtered_df = df.filter((df[\"AGE\"] > 30) | (df[\"CRIM\"] == \"HR\"))\n",
    "\n",
    "# NOT Condition (~):\n",
    "\n",
    "filtered_df = df.filter(~(df[\"RAD\"] == \"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48809722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|    B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3|396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8|396.9| 9.14| 21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"RAD\"].isin(\"1\", \"2\"))\n",
    "filtered_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42234afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889|39.0|5.4509|  5|311|   15.2| 390.5|15.71| 21.7|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456|36.6|3.7965|  4|307|   21.0|288.99|11.69| 20.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df[\"AGE\"].startswith(\"3\"))\n",
    "filtered_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7da4af",
   "metadata": {},
   "source": [
    "This code demonstrates filtering in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e3647a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 19:53:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         2|       Mohit|     DU|\n",
      "|         3|      Rohith|    BHU|\n",
      "|         4|     Sridevi|    LPU|\n",
      "|         5|     Gnanesh|    IIT|\n",
      "|         6|       Anita|     DU|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: college == 'DU'\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         2|       Mohit|     DU|\n",
      "|         6|       Anita|     DU|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: college == 'DU' AND student_ID == '1'\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: student_NAME starts with 'A' using SQL string filter\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         6|       Anita|     DU|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: college == 'DU' AND student_NAME == 'Amit' using col() function\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: student_ID in [1,2] OR college in ['DU','IIT']\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         2|       Mohit|     DU|\n",
      "|         5|     Gnanesh|    IIT|\n",
      "|         6|       Anita|     DU|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: student_NAME startswith 'A' AND endswith 't'\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: student_NAME contains 'ith'\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         3|      Rohith|    BHU|\n",
      "+----------+------------+-------+\n",
      "\n",
      "Filter: NOT college == 'DU'\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         3|      Rohith|    BHU|\n",
      "|         4|     Sridevi|    LPU|\n",
      "|         5|     Gnanesh|    IIT|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n",
      "+----------+------------+-------+\n",
      "|student_ID|student_NAME|college|\n",
      "+----------+------------+-------+\n",
      "|         1|        Amit|     DU|\n",
      "|         2|       Mohit|     DU|\n",
      "|         3|      Rohith|    BHU|\n",
      "|         4|     Sridevi|    LPU|\n",
      "|         5|     Gnanesh|    IIT|\n",
      "|         6|       Anita|     DU|\n",
      "|         7|       Amrit|    IIT|\n",
      "+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySparkFilteringExamples\").getOrCreate()\n",
    "\n",
    "# Sample data: list of tuples (student_ID, student_NAME, college)\n",
    "data = [\n",
    "    (\"1\", \"Amit\", \"DU\"),\n",
    "    (\"2\", \"Mohit\", \"DU\"),\n",
    "    (\"3\", \"Rohith\", \"BHU\"),\n",
    "    (\"4\", \"Sridevi\", \"LPU\"),\n",
    "    (\"5\", \"Gnanesh\", \"IIT\"),\n",
    "    (\"6\", \"Anita\", \"DU\"),\n",
    "    (\"7\", \"Amrit\", \"IIT\"),\n",
    "]\n",
    "\n",
    "# Define schema column names\n",
    "columns = [\"student_ID\", \"student_NAME\", \"college\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# 1. Filter rows with single condition: college = 'DU'\n",
    "print(\"Filter: college == 'DU'\")\n",
    "df.filter(df.college == \"DU\").show()\n",
    "\n",
    "# 2. Filter with multiple conditions using & (AND): college='DU' AND student_ID='1'\n",
    "print(\"Filter: college == 'DU' AND student_ID == '1'\")\n",
    "df.filter((df.college == \"DU\") & (df.student_ID == \"1\")).show()\n",
    "\n",
    "# 3. Filter using SQL expression string syntax: student_NAME starts with 'A'\n",
    "print(\"Filter: student_NAME starts with 'A' using SQL string filter\")\n",
    "df.filter(\"student_NAME LIKE 'A%'\").show()\n",
    "\n",
    "# 4. Filter using SQL col() function with multiple conditions: college='DU' AND student_NAME='Amit'\n",
    "print(\"Filter: college == 'DU' AND student_NAME == 'Amit' using col() function\")\n",
    "df.filter((col(\"college\") == \"DU\") & (col(\"student_NAME\") == \"Amit\")).show()\n",
    "\n",
    "# 5. Filter using isin(): student_ID in [1, 2], or college in ['DU', 'IIT']\n",
    "print(\"Filter: student_ID in [1,2] OR college in ['DU','IIT']\")\n",
    "df.filter((df.student_ID.isin([\"1\", \"2\"])) | (df.college.isin([\"DU\", \"IIT\"]))).show()\n",
    "\n",
    "# 6. Filter with startswith and endswith: student_NAME starts with 'A' AND ends with 't'\n",
    "print(\"Filter: student_NAME startswith 'A' AND endswith 't'\")\n",
    "df.filter((df.student_NAME.startswith(\"A\")) & (df.student_NAME.endswith(\"t\"))).show()\n",
    "\n",
    "# 7. Filter rows containing substring: student_NAME contains 'ith'\n",
    "print(\"Filter: student_NAME contains 'ith'\")\n",
    "df.filter(df.student_NAME.contains(\"ith\")).show()\n",
    "\n",
    "# 8. Using NOT condition: exclude students from college 'DU'\n",
    "print(\"Filter: NOT college == 'DU'\")\n",
    "df.filter(~(df.college == \"DU\")).show()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca754e8",
   "metadata": {},
   "source": [
    "Sample Code\n",
    "This code demonstrates grouping in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90b63f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 20:12:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/07 20:12:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/07 20:12:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|  Alice|        HR|  5000|\n",
      "|    Bob|        IT|  6000|\n",
      "|Charlie|   Finance|  7000|\n",
      "|  David|        IT|  6000|\n",
      "|    Eve|        HR|  5500|\n",
      "|  Frank|   Finance|  8000|\n",
      "+-------+----------+------+\n",
      "\n",
      "Group by Department - Count:\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|        HR|    2|\n",
      "|        IT|    2|\n",
      "|   Finance|    2|\n",
      "+----------+-----+\n",
      "\n",
      "Group by Department - Sum of Salaries:\n",
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|        HR|      10500|\n",
      "|        IT|      12000|\n",
      "|   Finance|      15000|\n",
      "+----------+-----------+\n",
      "\n",
      "Group by Department - Average Salary:\n",
      "+----------+-----------+\n",
      "|department|avg(salary)|\n",
      "+----------+-----------+\n",
      "|        HR|     5250.0|\n",
      "|        IT|     6000.0|\n",
      "|   Finance|     7500.0|\n",
      "+----------+-----------+\n",
      "\n",
      "Group by Department - Multiple Aggregates:\n",
      "+----------+--------------+------------+--------------+\n",
      "|department|employee_count|total_salary|average_salary|\n",
      "+----------+--------------+------------+--------------+\n",
      "|        HR|             2|       10500|        5250.0|\n",
      "|        IT|             2|       12000|        6000.0|\n",
      "|   Finance|             2|       15000|        7500.0|\n",
      "+----------+--------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum, count\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Grouping Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"HR\", 5000),\n",
    "    (\"Bob\", \"IT\", 6000),\n",
    "    (\"Charlie\", \"Finance\", 7000),\n",
    "    (\"David\", \"IT\", 6000),\n",
    "    (\"Eve\", \"HR\", 5500),\n",
    "    (\"Frank\", \"Finance\", 8000),\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show original data\n",
    "print(\"Original Data:\")\n",
    "df.show()\n",
    "\n",
    "# Group by department and calculate aggregates\n",
    "print(\"Group by Department - Count:\")\n",
    "df.groupBy(\"department\").count().show()\n",
    "\n",
    "print(\"Group by Department - Sum of Salaries:\")\n",
    "df.groupBy(\"department\").sum(\"salary\").show()\n",
    "\n",
    "print(\"Group by Department - Average Salary:\")\n",
    "df.groupBy(\"department\").agg(avg(\"salary\")).show()\n",
    "\n",
    "print(\"Group by Department - Multiple Aggregates:\")\n",
    "df.groupBy(\"department\").agg(\n",
    "    count(\"name\").alias(\"employee_count\"),\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    avg(\"salary\").alias(\"average_salary\")\n",
    ").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
