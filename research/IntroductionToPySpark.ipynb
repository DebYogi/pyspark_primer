{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323ba1bc",
   "metadata": {},
   "source": [
    "Every great PySpark adventure starts with a SparkSession. Think of it as your trusty sidekick, like Robin to Batman. Itâ€™s the entry point to everything Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58215f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/08 02:41:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark's in the house! ðŸ”¥\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initiate the SparkSession - you're basically summoning Spark's power!\n",
    "# For reproducible local runs in the workshops we set an explicit local master\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark 101\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark's in the house! ðŸ”¥\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693bc65d",
   "metadata": {},
   "source": [
    "DataFrames: Like pandas, but on steroids ðŸ’ª\n",
    "If youâ€™ve used pandas, you know DataFrames are your bread and butter for data manipulation. Well, guess what? PySpark has DataFrames too, but they can handle BIGGER data, and they do it with style. ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ae7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14| 21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03| 34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94| 33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33| 36.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading a CSV file into a PySpark DataFrame\n",
    "# Use relative data paths (repo root -> data/)\n",
    "df = spark.read.csv(\"data/boston.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a926bb8",
   "metadata": {},
   "source": [
    "Hereâ€™s the thing: PySpark DataFrames are like a friend who promises to do the dishes but waits until the very last second. They are lazy, which means they donâ€™t actually do any work until you tell them to. This is called lazy evaluation.Example: When you ask PySpark to do something, like filter some data:Nothing happens. Nada. Zilch. PySpark is chillinâ€™. ðŸ§˜â€â™‚ï¸\n",
    "\n",
    "But when you force it to act (by using an action, like .show() or .collect()), thatâ€™s when it rolls up its sleeves and does the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc346511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14| 21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03| 34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94| 33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33| 36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21| 28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43| 22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15| 27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93| 16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1| 18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45| 15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27| 18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71| 21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26| 20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26| 18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47| 19.9|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67| 17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69| 20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28| 18.2|\n",
      "|1.25179| 0.0| 8.14|   0|0.538| 5.57| 98.1|3.7979|  4|307|   21.0|376.57|21.02| 13.6|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where age is greater than 30\n",
    "df_filtered = df.filter(df['age'] > 30)\n",
    "\n",
    "# Now Spark gets off the couch and does something!\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3f63d",
   "metadata": {},
   "source": [
    "Transformations vs Actions: The Yin and Yang of PySpark\n",
    "Transformations: These are like making a to-do list. You say, \"Spark, I want you to select some columns, filter, and group this data!\" Spark nods but doesnâ€™t move just yet. These are lazy operations.\n",
    "\n",
    "Examples: .filter(), .select(), .groupBy()\n",
    "Actions: This is you shouting, \"DO THE THING!\" And Spark, like a well-trained dog, jumps into action. These are the operations that trigger Spark to actually execute your transformations.\n",
    "\n",
    "Examples: .show(), .count(), .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|TAX|PTRATIO|     B|LSTAT|Price|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98| 24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14| 21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03| 34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94| 33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33| 36.2|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-----+\n",
      "|RAD|count|\n",
      "+---+-----+\n",
      "|  1|   13|\n",
      "|  6|   22|\n",
      "|  3|   30|\n",
      "|  5|  105|\n",
      "|  4|   86|\n",
      "|  8|   21|\n",
      "|  7|   10|\n",
      "| 24|  132|\n",
      "|  2|   23|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read employee data\n",
    "df = spark.read.csv(\"data/boston.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.show(5)\n",
    "\n",
    "# Filter employees over 30 years old\n",
    "adults = df.filter(df['AGE'] > 30)\n",
    "\n",
    "# Group by department and count how many employees are in each department\n",
    "RAD_count = adults.groupBy(\"RAD\").count()\n",
    "\n",
    "# Show the result\n",
    "RAD_count.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060370b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.8.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
